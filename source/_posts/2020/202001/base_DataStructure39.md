---
title: DataStructure_基础 (39)
date: 2020-01-16
tags: DataStructure
toc: true
---

### 哈希算法
    之前对哈希的理解很模糊, 不经问, 所以抽时间重新看一遍

<!-- more -->

#### 哈希算法的基本含义
> 是根据键(Key)而直接访问在内存存储位置的数据结构.也就是说, 它通过计算一个关于键值的函数, 将所需查询的数据映射到表中一个位置来访问记录, 这加快了查找速度.这个映射函数称做散列函数, 存放记录的数组称做散列表

#### 散列函数
> 散列函数, 顾名思义, 它是一个函数.如果把它定义成 hash(key) , 其中 key 表示元素的键值, 则 hash(key) 的值表示经过散列函数计算得到的散列值
- 散列函数的特点
    * 确定性
        * 如果两个散列值是不相同的(根据同一函数), 那么这两个散列值的原始输入也是不相同的.
    * 散列碰撞(collision)
        * 散列函数的输入和输出不是唯一对应关系的, 如果两个散列值相同, 两个输入值很可能是相同的, 但也可能不同.
    * 不可逆性
        * 一个哈希值对应无数个明文, 理论上你并不知道哪个是
    * 混淆特性
        * 输入一些数据计算出散列值, 然后部分改变输入值, 一个具有强混淆特性的散列函数会产生一个完全不同的散列值

#### 哈希冲突
> 再好的散列函数都无法避免散列冲突
> 抽屉原理: 桌上有十个苹果, 要把这十个苹果放到九个抽屉里, 无论怎样放, 我们会发现至少会有一个抽屉里面至少放两个苹果.这一现象就是我们所说的“抽屉原理”
- 解决哈希冲突的方法
    * 开放寻址法
        * 当我们往散列表中插入数据时, 如果某个数据经过散列函数散列之后, 存储位置已经被占用了, 我们就从当前位置开始, 依次往后查找, 看是否有空闲位置, 直到找到为止
        * 弊端就是当散列表中插入的数据越来越多时, 散列冲突发生的可能性就会越来越大, 空闲位置会越来越少, 线性探测的时间就会越来越久.极端情况下, 需要从头到尾探测整个散列表, 所以最坏情况下的时间复杂度为 O(n)
    * 链表法
        * 在散列表中, 每个位置对应一条链表, 所有散列值相同的元素都放到相同位置对应的链表中

#### 分布式集群数据存储
> 随着系统数据量的增加, 单节点的存储空间会很快占满, 单节点集中存储带来的经济成本和风险也越来越高.基于以上的考虑, 现在企业存储纷纷采用分布式存储, 将数据存储在多个配置相同的服务器上, 组成分布式集群(省钱, 扩容方便, 风险小)那么, 存储数据从一个节点变为多个分布式节点的时候, 有什么路由算法能保证同一数据的读写都能在一个节点上呢
- hash取模的算法
    * 对请求的key进行hash运算后, 得到一个value值, 用value值去和服务器数量进行取模, 得到的结果就对应那一台机器
    * 在没有新增或者删除服务器的时候, 取模hash的方法可以均匀的实现数据的分配, 一旦增减节点, 则会导致全部数据查不到, 需要重新存储
- 一致性哈希
    * 一致性hash算法也是取模运算, 只是取模的对象不一样, 简单取模hash是对服务器数量进行取模, 而一致性hash是对2^32取模
    * 假设存在一个虚拟的哈希环, 他的值空间为0-2^32-1(即哈希值是一个32位无符号整形)
    * 假设我们对服务器D0、服务器D1、服务器D2、服务器D3分别进行哈希计算之后, 让节点落在哈希环上, 
    * ![一致性哈希_哈希环](/img/20200116_1.png)
    * 需要通过数据 key 找到对应的服务器然后存储了, 我们约定,通过数据 key 的哈希值落在哈希环上的节点, 如果命中了机器节点就落在这个机器上, 否则落在顺时针直到碰到第一个机器.如下图所示 : A 的哈希值落在了 D2 节点的前面, 往下找落在了 D2 机器上, D的哈希值 在 D1 节点的前面, 往下找到了 D1 机器, B的哈希值刚好落在了D1 节点上
    * ![一致性哈希_数据存储](/img/20200116_2.png)
- 一致性哈希的分析
    * 一致性哈希主要就是解决当机器减少或增加的时候, 大面积的数据重新哈希的问题, 主要从下面 2 个方向去考虑的, 当节点宕机时, 数据记录会被定位到下一个节点上, 当新增节点的时候 , 相关区间内的数据记录就需要重新哈希
    * 某节点宕机
        * 假设上图中的 节点 D2 因为一些原因宕机了,可以看到, 只有数据 A 的记录需要重新重新定位存储到节点 D1 上, 因为 D1 是 D2 的下一个节点, 其它的数据都没有被影响到, 此时被影响的仅仅是 图中的 D0-D2 这段区间的记录, 也就是之前落在 D2 上的数据现在都要落到 D1 上面了
        * ![一致性哈希_节点宕机](/img/20200116_3.png)
    * 新增机器
        * 假设需要增加一台机器, 也就是增加一个节点D4, 如下图所示, 这个节点落在 D2-D1 之间, 按照上述的哈希环上的哈希值落在节点的规则, 那么此时之前落在 D2 到 D4 之间的数据都需要重新定位到新的节点上面了, 而其它位置的数据是不需要有改变的
        * ![一致性哈希_新增节点](/img/20200116_4.png)
- 一致性哈希的数据倾斜问题
    * 一致性Hash算法在服务节点太少时, 容易因为节点分部不均匀而造成数据倾斜(被缓存的对象大部分集中缓存在某一台服务器上)问题.比如只有 2 台机器, 这 2 台机器离的很近, 那么顺时针第一个机器节点上将存在大量的数据, 第二个机器节点上数据会很少.如下图所示, D0 机器承载了绝大多数的数据
    * ![一致性哈希_数据倾斜](/img/20200116_5.png)
    * 解决方式: 创建虚拟节点
    * 为了避免出现数据倾斜问题, 一致性 Hash 算法引入了虚拟节点的机制, 也就是每个机器节点会进行多次哈希, 最终每个机器节点在哈希环上会有多个虚拟节点存在, 使用这种方式来大大削弱甚至避免数据倾斜问题.同时数据定位算法不变, 只是多了一步虚拟节点到实际节点的映射, 例如定位到“D1#1”、“D1#2”、“D1#3”三个虚拟节点的数据均定位到 D1 上.这样就解决了服务节点少时数据倾斜的问题
    * ![一致性哈希_虚拟节点](/img/20200116_6.png)







